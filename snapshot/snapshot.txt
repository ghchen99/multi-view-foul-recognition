Directory Structure:
===================

app.py
inference_pipeline.py
models
├── ActionData.py
└── Decoder.py
requirements.txt
snapshot.py
c
training_pipeline.py
utils
├── FeatureExtractor.py
├── FoulDataPreprocessor.py
├── HDF5Reader.py
└── training.py

File Contents:
=============


--- app.py ---
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import tempfile
from inference_pipeline import FoulInferencePipeline
import logging
from typing import Tuple, Dict, Any, List

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class VideoProcessingError(Exception):
    """Custom exception for video processing errors"""
    pass

class APIError(Exception):
    """Base exception for API errors"""
    def __init__(self, message: str, status_code: int = 400):
        super().__init__(message)
        self.status_code = status_code

app = Flask(__name__)
CORS(app)

# Configuration
MAX_CONTENT_LENGTH = 100 * 1024 * 1024  # 100MB max file size
ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'wmv'}
app.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH

def allowed_file(filename: str) -> bool:
    """Check if the file extension is allowed"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def validate_video_file(file) -> None:
    """Validate the uploaded video file"""
    if not file:
        raise APIError('No file provided', 400)
    
    if file.filename == '':
        raise APIError('No selected file', 400)
        
    if not allowed_file(file.filename):
        raise APIError(f'Invalid file type. Allowed types: {", ".join(ALLOWED_EXTENSIONS)}', 400)

def format_predictions(raw_predictions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Format predictions from the new model structure into the existing API response format.
    The new model returns a list of dictionaries with category, prediction, and probability.
    """
    try:
        formatted_predictions = []
        for pred in raw_predictions:
            category_result = {
                'category': pred['category'],
                'details': [{
                    'prediction': str(pred['prediction']),
                    'probability': float(pred['probability'])
                }]
            }
            formatted_predictions.append(category_result)
            
            # Log in a nice format
            logger.info(f"\n{pred['category']}:")
            logger.info(f"  • {pred['prediction']:<20} {pred['probability']:.2%}")
                
        return {'predictions': formatted_predictions}
        
    except Exception as e:
        logger.error(f"Error formatting predictions structure: {str(e)}")
        logger.debug(f"Raw predictions structure: {raw_predictions}")
        raise

def process_video(video_file) -> Tuple[str, Dict[str, Any]]:
    """Process the video file and return predictions"""
    temp_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as temp_file:
            video_file.save(temp_file.name)
            temp_path = temp_file.name

        try:
            logger.info("Processing video for inference...")
            features_path = pipeline.process_video_for_inference(temp_path, replay_speed=1.4)
            
            logger.info("Running inference...")
            raw_predictions = pipeline.inference(features_path)
            
            # Format predictions using the enhanced formatter
            formatted_predictions = format_predictions(raw_predictions)
            
            logger.info("Inference completed successfully!")
            return features_path, formatted_predictions
            
        except Exception as e:
            raise VideoProcessingError(f"Error processing video: {str(e)}")
            
    except Exception as e:
        raise APIError(f"Error saving video file: {str(e)}", 500)
    finally:
        # Clean up temporary file if it exists
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except Exception as e:
                logger.error(f"Error cleaning up temporary file: {str(e)}")

def cleanup_files(*files: str) -> None:
    """Clean up temporary files"""
    for file_path in files:
        try:
            if file_path and os.path.exists(file_path):
                os.unlink(file_path)
        except Exception as e:
            logger.error(f"Error cleaning up file {file_path}: {str(e)}")

# Initialize pipeline with error handling
try:
    pipeline = FoulInferencePipeline(model_path="pretrained_models/20250128_223835/foul_detection_model.pth")
except Exception as e:
    logger.critical(f"Failed to initialize inference pipeline: {str(e)}")
    raise

@app.errorhandler(413)
def request_entity_too_large(error):
    return jsonify({
        'error': f'File too large. Maximum size is {MAX_CONTENT_LENGTH // (1024 * 1024)}MB'
    }), 413

@app.route('/api/inference', methods=['POST'])
def process_video_endpoint():
    temp_path = None
    features_path = None
    
    try:
        # Validate request
        if not request.files:
            raise APIError('No file part in the request', 400)
            
        video_file = request.files.get('video')
        validate_video_file(video_file)
        
        # Process video
        features_path, predictions = process_video(video_file)
        
        return jsonify({
            'status': 'success',
            **predictions  # Unpack formatted predictions
        })
        
    except APIError as e:
        logger.warning(f"API Error: {str(e)}")
        return jsonify({'error': str(e)}), e.status_code
        
    except VideoProcessingError as e:
        logger.error(f"Video Processing Error: {str(e)}")
        return jsonify({'error': 'Error processing video'}), 500
        
    except Exception as e:
        logger.error(f"Unexpected Error: {str(e)}")
        return jsonify({'error': 'An unexpected error occurred'}), 500
        
    finally:
        # Clean up temporary files in all cases
        cleanup_files(temp_path, features_path)

if __name__ == '__main__':
    app.run(debug=True, port=5000)

--- inference_pipeline.py ---
import os
import logging
from pathlib import Path
import torch
from typing import Dict, Tuple
import torch.nn.functional as F

from utils.FeatureExtractor import FeatureExtractor
from models.ActionData import ActionData
from utils.HDF5Reader import save_to_hdf5
from utils.FoulDataPreprocessor import FoulDataPreprocessor
from models.Decoder import Decoder
from utils.training import ImprovedMultiTaskModel, load_model

class FoulInferencePipeline:
    """Pipeline for running inference with the improved foul detection model."""
    
    def __init__(self, model_path: str, base_dir: str = 'data/dataset/'):
        self.base_dir = Path(base_dir)
        self.preprocessor = FoulDataPreprocessor()
        self.decoder = Decoder()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.metadata, self.class_weights, _, self.scaler = self._load_model(model_path)
        self.extractor = FeatureExtractor(base_dir=base_dir, model_type='r3d_18')
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        logging.info(f"Model loaded successfully. Device: {self.device}")
        if self.scaler:
            logging.info("Input scaler loaded")

    def _load_model(self, model_path: str) -> Tuple[ImprovedMultiTaskModel, dict, dict, dict, object]:
        """Load the trained model and its components."""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
            
        model, metadata, class_weights, history, scaler = load_model(model_path, self.device)
        model.eval()
        
        logging.info("Model architecture:")
        for task, head in model.primary_heads.items():
            out_features = head.net[-1].out_features
            logging.info(f"- {task}: {out_features} classes")
        logging.info(f"- severity: {model.severity_head[-1].out_features} classes")
        
        return model, metadata, class_weights, history, scaler

    def process_video_for_inference(self, video_path: str, replay_speed: float = 1.0) -> str:
        """Process a single video for inference."""
        output_dir = self.base_dir / 'inference'
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / 'inference_features.h5'
        
        logging.info(f"Processing video: {video_path}")
        
        # Create a dummy action with default values for preprocessing
        dummy_action = ActionData({
            'Offence': 'Offence',
            'Bodypart': 'Under body',  
            'Action class': 'Tackling',
            'Touch ball': 'No',
            'Try to play': 'Yes',
            'Severity': '1.0',
            'Clips': []
        })
        
        # Extract features
        features = self.extractor.extract_video_features(video_path)
        if features is None:
            raise ValueError("Failed to extract video features")
            
        # Add clip with features
        dummy_action.clips = [{
            'video_features': features,
            'Replay speed': replay_speed,
            'Camera type': 'Close-up player or field referee'
        }]
        
        logging.info("Features extracted successfully")
        
        # Save features
        save_to_hdf5([dummy_action], str(output_file))
        logging.info(f"Saved features to: {output_file}")
        
        return str(output_file)

    def inference(self, inference_file: str) -> Dict:
        """Run inference on processed video data."""
        logging.info("Starting inference...")
        
        if not os.path.exists(inference_file):
            raise FileNotFoundError(f"Inference file not found: {inference_file}")
            
        try:
            # Process data
            processed_data = self.preprocessor.process_data(inference_file)
            if processed_data is None:
                raise ValueError("Preprocessor returned None")
                
            X_test, _ = processed_data
            if X_test is None:
                raise ValueError("X_test is None after preprocessing")
                
            # Apply scaler if available
            if self.scaler:
                X_test = self.scaler.transform(X_test)
            
            # Convert to tensor and move to device
            X_test = torch.FloatTensor(X_test).to(self.device)
            logging.info(f"Input shape for inference: {X_test.shape}")
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(X_test)
                
                # Apply softmax to get probabilities
                predictions = {
                    task: F.softmax(output, dim=1).cpu().numpy()
                    for task, output in outputs.items()
                }
            
            # Decode predictions
            decoded_predictions = []
            
            # Map task names to decoder maps
            task_to_map = {
                'actionclass': self.decoder.action_class_map,
                'bodypart': self.decoder.bodypart_map,
                'offence': self.decoder.offence_map,
                'touchball': self.decoder.touchball_map,
                'trytoplay': self.decoder.trytoplay_map,
                'severity': self.decoder.severity_map
            }
            
            # Nice display names for tasks
            display_names = {
                'actionclass': 'Action Class',
                'bodypart': 'Body Part',
                'offence': 'Offence',
                'touchball': 'Touch Ball',
                'trytoplay': 'Try to Play',
                'severity': 'Severity'
            }
            
            for task, probs in predictions.items():
                # Get only the top probability
                top_prob, top_idx = torch.max(torch.from_numpy(probs[0]), dim=0)
                
                # Get the mapping for this task
                task_map = task_to_map[task]
                
                # Decode prediction using the appropriate map
                decoded_class = task_map[top_idx.item()].decode('utf-8')
                
                category_pred = {
                    'category': display_names[task],
                    'prediction': decoded_class,
                    'probability': top_prob.item()
                }
                decoded_predictions.append(category_pred)
            
            return decoded_predictions
            
        except Exception as e:
            logging.error(f"Error during inference: {str(e)}")
            raise

def main():
    """Run the inference pipeline on a test video."""
    model_path = "pretrained_models/20250128_223835/foul_detection_model.pth"
    pipeline = FoulInferencePipeline(model_path)
    
    try:
        # Process and run inference on a test video
        video_path = 'data/dataset/inference/testaction5_clip1.mp4'
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Test video not found: {video_path}")
            
        logging.info("Processing video for inference...")
        inference_features = pipeline.process_video_for_inference(
            video_path,
            replay_speed=1.4
        )
        
        logging.info("Running inference...")
        predictions = pipeline.inference(inference_features)

        # Print predictions in a nice format
        print("\nDecoded Predictions with Probabilities:")
        print("=" * 50)
        for category in predictions:
            confidence = "▓" * int(category['probability'] * 20) + "░" * (20 - int(category['probability'] * 20))
            print(f"\n{category['category'].upper()}:")
            print(f"  • {category['prediction']:<20} [{confidence}] {category['probability']:.1%}")
        
        logging.info("Inference pipeline completed successfully!")
        
    except Exception as e:
        logging.error(f"Inference pipeline failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()

--- models/ActionData.py ---
import logging
from typing import Dict


# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ActionData class to store and process action parameters
class ActionData:
    """
    Represents a specific action in the dataset, including associated clips and features.
    """

    def __init__(self, action_data: Dict):
        """
        Initialises an ActionData object with the provided action data.
        Sets self.valid to False if the action should be skipped, except for "No offence" cases
        where empty values are populated with "missing".

        Parameters:
        action_data (dict): A dictionary containing detailed information about the action. 
        """
        # Initialize validity flag
        self.valid = True
        
        # Special handling for "No offence" cases
        is_no_offence = action_data['Offence'] == "No offence"
        
        # Check and store offence - skip if empty string and not "No offence"
        if action_data['Offence'] == "" and not is_no_offence:
            logging.warning("Skipping action: Empty offence value")
            self.valid = False
            return
            
        # Check and store bodypart - skip if empty string and not "No offence"
        if action_data['Bodypart'] == "":
            logging.warning("Skipping action: Empty bodypart value")
            self.valid = False
            return
            
        # Check and store actionclass - skip if empty string or Dont know and not "No offence"
        if action_data['Action class'] in ["", "Dont know"]:
            logging.warning("Skipping action: Invalid action class value")
            self.valid = False
            return

        # Store offence value
        self.offence = action_data['Offence']
        
        # Handle bodypart and upperbodypart logic
        if action_data['Bodypart'] == "":
            self.bodypart = "" if is_no_offence else action_data['Bodypart']
        elif action_data['Bodypart'] == 'Upper body':
            upperbodypart = action_data['Upper body part']
            # Only update if upperbodypart is not empty string
            if upperbodypart != "":
                # Convert 'Use of shoulders' to 'Use of shoulder'
                if upperbodypart == 'Use of shoulders':
                    self.bodypart = 'Use of shoulder'
                else:
                    self.bodypart = upperbodypart
            else:
                self.bodypart = 'Upper body'  # Keep original value if upperbodypart is empty string
        else:
            self.bodypart = action_data['Bodypart']
        
        # Store actionclass - use "missing" for empty values in "No offence" cases
        self.actionclass = "" if (action_data['Action class'] == "" and is_no_offence) else action_data['Action class']
        
        # Store severity - use "missing" for empty values in "No offence" cases
        self.severity = "1.0" if (action_data['Severity'] == "") else action_data['Severity']
        
        # Store trytoplay - convert empty string to No or "missing" for "No offence" cases
        self.trytoplay = "No" if (action_data['Try to play'] == "" and is_no_offence) else ('No' if action_data['Try to play'] == "" else action_data['Try to play'])
        
        # Store touchball - convert empty string to No or "missing" for "No offence" cases
        self.touchball = "No" if (action_data['Touch ball'] == "" and is_no_offence) else ('No' if action_data['Touch ball'] == "" else action_data['Touch ball'])
        
        # Store clips
        self.clips = action_data['Clips']




--- models/Decoder.py ---
import torch

class Decoder:
    def __init__(self):
        # Define mappings for decoding
        self.action_class_map = {
            int(v): k.encode('utf-8') for k, v in {
                'Standing tackling': '0', 'Tackling': '1', 'Holding': '2',
                'Challenge': '3', 'Elbowing': '4', 'High leg': '5',
                'Pushing': '6', 'Dive': '7'
            }.items()
        }
        
        self.bodypart_map = {
            int(v): k.encode('utf-8') for k, v in {
                'Under body': '0', 'Use of arms': '1',
                'Use of shoulder': '2', 'Upper body': '3'
            }.items()
        }

        self.offence_map = {
            int(v): k.encode('utf-8') for k, v in {
                'No offence': '0', 'Between': '1', 'Offence': '2'
            }.items()
        }
        
        self.touchball_map = {
            int(v): k.encode('utf-8') for k, v in {
                'No': '0', 'Maybe': '1', 'Yes': '2'
            }.items()
        }

        self.trytoplay_map = {
            int(v): k.encode('utf-8') for k, v in {
                'No': '0', 'Yes': '1'
            }.items()
        }
        
        self.severity_map = {
            int(v): f"{float(k):.1f}{' No card' if k == '1.0' else ' Borderline No/Yellow' if k == '2.0' else ' Yellow card' if k == '3.0' else ' Yellow/ borderline Red' if k == '4.0' else ' Red card'}".encode('utf-8')
            for k, v in {
                '1.0': '0', '2.0': '1', '3.0': '2', '4.0': '3', '5.0': '4'
            }.items()
        }

    def get_predictions_and_probs(self, predictions):
        """
        Helper method to get the predicted labels and their corresponding probabilities.
        """
        probs = torch.softmax(predictions, dim=1)
        labels = torch.argmax(probs, dim=1)
        max_probs = probs.max(dim=1).values
        return labels, max_probs

    def decode_predictions(self, actionclass_pred, bodypart_pred, offence_pred, touchball_pred, trytoplay_pred, severity_pred):
        """
        Decodes predictions and returns them alongside their probabilities in an array.
        
        Returns:
            list: Array of dictionaries containing decoded predictions and their probabilities
        """
        # Get predictions and probabilities
        actionclass_pred_labels, actionclass_probs = self.get_predictions_and_probs(actionclass_pred)
        bodypart_pred_labels, bodypart_probs = self.get_predictions_and_probs(bodypart_pred)
        offence_pred_labels, offence_probs = self.get_predictions_and_probs(offence_pred)
        touchball_pred_labels, touchball_probs = self.get_predictions_and_probs(touchball_pred)
        trytoplay_pred_labels, trytoplay_probs = self.get_predictions_and_probs(trytoplay_pred)
        severity_pred_labels, severity_probs = self.get_predictions_and_probs(severity_pred)

        # Decode predictions
        actionclass = [self.action_class_map[label.item()] for label in actionclass_pred_labels]
        bodypart = [self.bodypart_map[label.item()] for label in bodypart_pred_labels]
        offence = [self.offence_map[label.item()] for label in offence_pred_labels]
        touchball = [self.touchball_map[label.item()] for label in touchball_pred_labels]
        trytoplay = [self.trytoplay_map[label.item()] for label in trytoplay_pred_labels]
        severity = [self.severity_map[label.item()] for label in severity_pred_labels]

        # Create results array
        results = [
            {
                "category": "Action Class",
                "predictions": [x.decode('utf-8') for x in actionclass],
                "probabilities": actionclass_probs
            },
            {
                "category": "Body Part",
                "predictions": [x.decode('utf-8') for x in bodypart],
                "probabilities": bodypart_probs
            },
            {
                "category": "Offence",
                "predictions": [x.decode('utf-8') for x in offence],
                "probabilities": offence_probs
            },
            {
                "category": "Touch Ball",
                "predictions": [x.decode('utf-8') for x in touchball],
                "probabilities": touchball_probs
            },
            {
                "category": "Try to Play",
                "predictions": [x.decode('utf-8') for x in trytoplay],
                "probabilities": trytoplay_probs
            },
            {
                "category": "Severity",
                "predictions": [x.decode('utf-8') for x in severity],
                "probabilities": severity_probs
            }
        ]

        return results

--- requirements.txt ---
SoccerNet
python-dotenv
opencv-python-headless
seaborn
h5py
torch
torchvision
numpy<2.0
flask 
flask-cors 
scikit-learn

--- snapshot.py ---
import os
import fnmatch
from pathlib import Path
from typing import List, Set, Tuple

DEFAULT_IGNORE_PATTERNS = {
    '.git/',
    '.git/**',
    '.DS_Store',
    '__pycache__/',
    '**/__pycache__/',
    '*.pyc',
    '.env',
    'node_modules/',
    '.idea/',
    '.vscode/',
    '.pytest_cache/',
    '*.swp',
    '.gitignore'
    'venv/',
    '.venv/',
    'data/',
    'pretrained_models/',
    'snapshot/',
    'video-foul-detection/',
}

def parse_gitignore() -> Set[str]:
    """Parse .gitignore file and return a set of patterns."""
    ignore_patterns = DEFAULT_IGNORE_PATTERNS.copy()
    try:
        with open('.gitignore', 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # Handle directory patterns
                    if line.endswith('/'):
                        ignore_patterns.add(line)
                        ignore_patterns.add(f"{line}**")
                    # Handle wildcards
                    if '*' in line:
                        ignore_patterns.add(line)
                    else:
                        ignore_patterns.add(line)
                        ignore_patterns.add(f"**/{line}")
    except FileNotFoundError:
        pass
    return ignore_patterns

def should_ignore(path: str, ignore_patterns: Set[str]) -> bool:
    """
    Check if a path should be ignored based on patterns.
    Handles both file and directory patterns.
    """
    # Convert path to use forward slashes for consistency
    path = path.replace(os.sep, '/')
    
    # Always ignore hidden files and directories (starting with .)
    if os.path.basename(path).startswith('.'):
        return True

    for pattern in ignore_patterns:
        # Handle directory-specific patterns
        if pattern.endswith('/') and os.path.isdir(path):
            if fnmatch.fnmatch(f"{path}/", pattern):
                return True
        # Handle patterns with wildcards
        if '**' in pattern:
            # Replace ** with a reasonable pattern that matches any number of directories
            pattern_regex = pattern.replace('**', '*')
            if fnmatch.fnmatch(path, pattern_regex):
                return True
        # Direct pattern matching
        if fnmatch.fnmatch(path, pattern) or fnmatch.fnmatch(os.path.basename(path), pattern):
            return True
        # Check if the path or any of its parent directories match the pattern
        path_parts = path.split('/')
        for i in range(len(path_parts)):
            subpath = '/'.join(path_parts[:i+1])
            if fnmatch.fnmatch(subpath, pattern):
                return True
    return False

def is_binary(file_path: str) -> bool:
    """Check if a file is binary by reading its first few thousand bytes."""
    try:
        with open(file_path, 'rb') as f:
            chunk = f.read(4096)
            if not chunk:  # empty file
                return False
            
            textchars = bytearray({7,8,9,10,12,13,27} | set(range(0x20, 0x100)) - {0x7f})
            return bool(chunk.translate(None, textchars))
    except (IOError, OSError):
        return True

def read_file_content(file_path: str) -> str:
    """Attempt to read file content with various encodings."""
    if is_binary(file_path):
        return "[Binary file]"

    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return f.read()
        except (UnicodeDecodeError, UnicodeError):
            continue
        except (IOError, OSError):
            return "[Error: Unable to read file]"
    
    return "[Error: Unable to decode file with available encodings]"

def generate_tree(path: str = ".", level: int = 0, ignore_patterns: Set[str] = None) -> Tuple[List[str], List[Tuple[str, str]]]:
    """
    Generate a tree structure of the directory and collect file contents.
    Returns a tuple of (tree_lines, file_contents).
    """
    if ignore_patterns is None:
        ignore_patterns = parse_gitignore()

    tree_lines = []
    file_contents = []
    
    try:
        items = sorted(os.listdir(path))
    except PermissionError:
        return [], []

    # Count items that aren't ignored for proper tree formatting
    valid_items = [item for item in items if not should_ignore(os.path.join(path, item), ignore_patterns)]
    last_idx = len(valid_items) - 1

    for idx, item in enumerate(items):
        full_path = os.path.join(path, item)
        rel_path = os.path.relpath(full_path)

        if should_ignore(rel_path, ignore_patterns):
            continue

        # Determine if this is the last item for proper tree formatting
        is_last = idx >= last_idx
        # Create tree line with proper formatting
        if level == 0:
            prefix = ""
        else:
            prefix = "└── " if is_last else "├── "
            prefix = "    " * (level - 1) + prefix

        tree_lines.append(f"{prefix}{item}")

        if os.path.isdir(full_path):
            # Recursively process directory
            subtree_lines, subtree_contents = generate_tree(full_path, level + 1, ignore_patterns)
            tree_lines.extend(subtree_lines)
            file_contents.extend(subtree_contents)
        else:
            # Read and store file content
            content = read_file_content(full_path)
            file_contents.append((rel_path, content))

    return tree_lines, file_contents

def create_snapshot(output_file: str = "snapshot.txt"):
    """Create a snapshot of the current directory and save it to a file."""
    tree_lines, file_contents = generate_tree()

    with open(output_file, 'w', encoding='utf-8') as f:
        # Write directory tree
        f.write("Directory Structure:\n")
        f.write("===================\n\n")
        for line in tree_lines:
            f.write(line + "\n")
        
        # Write file contents
        f.write("\nFile Contents:\n")
        f.write("=============\n\n")
        for filepath, content in file_contents:
            f.write(f"--- {filepath} ---\n")
            f.write(content)
            f.write("\n\n")

if __name__ == "__main__":
    create_snapshot()
    print("Snapshot has been created in 'snapshot.txt'")

--- test_pipeline.py ---
import os
import logging
from pathlib import Path
import torch
from typing import Dict, List, Optional, Tuple
import torch.nn.functional as F
import numpy as np
from datetime import datetime
from sklearn.metrics import classification_report, confusion_matrix

from utils.FeatureExtractor import FeatureExtractor
from utils.FoulDataPreprocessor import FoulDataPreprocessor
from utils.training import ImprovedMultiTaskModel, load_model

class FoulTestPipeline:
    """Pipeline for testing the foul detection model on a test dataset."""
    
    def __init__(self, model_path: str, base_dir: str = 'data/dataset/'):
        self.base_dir = Path(base_dir)
        self.preprocessor = FoulDataPreprocessor()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.metadata, self.class_weights, _, self.scaler = self._load_model(model_path)
        self.feature_extractor = FeatureExtractor(base_dir=base_dir, model_type='r3d_18')
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        logging.info(f"Model loaded successfully. Device: {self.device}")
        if self.scaler:
            logging.info("Input scaler loaded")

    def _load_model(self, model_path: str) -> Tuple[ImprovedMultiTaskModel, dict, dict, dict, object]:
        """Load the trained model and its components."""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
            
        model, metadata, class_weights, history, scaler = load_model(model_path, self.device)
        model.eval()
        
        logging.info("Model architecture:")
        for task, head in model.primary_heads.items():
            out_features = head.net[-1].out_features
            logging.info(f"- {task}: {out_features} classes")
        logging.info(f"- severity: {model.severity_head[-1].out_features} classes")
        
        return model, metadata, class_weights, history, scaler

    def extract_features(self, split: str, max_actions: Optional[int] = None) -> str:
        """Extract features for the test dataset split."""
        return self.feature_extractor.extract_features(split, max_actions)

    def evaluate(self, test_file: str) -> Dict:
        """Run evaluation on the test dataset and compute metrics."""
        logging.info("Starting test evaluation...")
        
        if not os.path.exists(test_file):
            raise FileNotFoundError(f"Test file not found: {test_file}")
            
        try:
            # Process test data
            X_test, y_test = self.preprocessor.process_data(test_file)
            if X_test is None or y_test is None:
                raise ValueError("Preprocessor returned None")
                
            # Apply scaler if available
            if self.scaler:
                X_test = self.scaler.transform(X_test)
            
            # Convert to tensor and move to device
            X_test = torch.FloatTensor(X_test).to(self.device)
            logging.info(f"Input shape for testing: {X_test.shape}")
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(X_test)
                predictions = {
                    task: F.softmax(output, dim=1).cpu().numpy()
                    for task, output in outputs.items()
                }
            
            # Compute metrics for each task
            metrics = {}
            task_labels = {
                'actionclass': 'Action Class',
                'bodypart': 'Body Part',
                'offence': 'Offence',
                'touchball': 'Touch Ball',
                'trytoplay': 'Try to Play',
                'severity': 'Severity'
            }
            
            for task, probs in predictions.items():
                # Get predicted classes
                pred_classes = np.argmax(probs, axis=1)
                true_classes = y_test[task].cpu().numpy()
                
                # Generate classification report
                task_report = classification_report(
                    true_classes, 
                    pred_classes,
                    output_dict=True
                )
                
                # Generate confusion matrix
                conf_matrix = confusion_matrix(true_classes, pred_classes)
                
                metrics[task_labels[task]] = {
                    'classification_report': task_report,
                    'confusion_matrix': conf_matrix,
                    'accuracy': task_report['accuracy']
                }
            
            # Save detailed results
            self._save_test_results(metrics)
            
            return metrics
            
        except Exception as e:
            logging.error(f"Error during test evaluation: {str(e)}")
            raise

    def _save_test_results(self, metrics: Dict) -> None:
        """Save detailed test results to a file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = Path("test_results") / timestamp
        results_dir.mkdir(parents=True, exist_ok=True)
        
        with open(results_dir / 'test_metrics.txt', 'w') as f:
            f.write("Test Evaluation Results\n")
            f.write("=" * 50 + "\n\n")
            
            for task, task_metrics in metrics.items():
                f.write(f"\n{task.upper()}\n")
                f.write("-" * len(task) + "\n\n")
                
                # Write accuracy
                f.write(f"Accuracy: {task_metrics['accuracy']:.4f}\n\n")
                
                # Write classification report
                f.write("Classification Report:\n")
                report = task_metrics['classification_report']
                for label, values in report.items():
                    if isinstance(values, dict):
                        f.write(f"\nClass {label}:\n")
                        for metric, value in values.items():
                            f.write(f"  {metric}: {value:.4f}\n")
                
                # Write confusion matrix
                f.write("\nConfusion Matrix:\n")
                np.savetxt(f, task_metrics['confusion_matrix'], fmt='%d')
                f.write("\n" + "=" * 50 + "\n")
        
        logging.info(f"Test results saved to {results_dir}")

def main():
    """Run the test pipeline."""
    model_path = "pretrained_models/20250128_223835/foul_detection_model.pth"
    pipeline = FoulTestPipeline(model_path)
    
    try:
        # Extract features for test set
        logging.info("Extracting test features...")
        # test_features = pipeline.extract_features('test', max_actions=None)
        
        test_features = 'data/dataset/test/test_features.h5'
        
        # Run evaluation
        logging.info("Running test evaluation...")
        metrics = pipeline.evaluate(test_features)
        
        # Print summary metrics
        print("\nTest Results Summary:")
        print("=" * 50)
        for task, task_metrics in metrics.items():
            print(f"\n{task}:")
            print(f"Accuracy: {task_metrics['accuracy']:.2%}")
        
        logging.info("Test pipeline completed successfully!")
        
    except Exception as e:
        logging.error(f"Test pipeline failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()

--- training_pipeline.py ---
import os
import logging
from pathlib import Path
import torch
from typing import Optional, Dict
from datetime import datetime

from utils.FeatureExtractor import FeatureExtractor
from utils.FoulDataPreprocessor import FoulDataPreprocessor
from utils.training import ImprovedMultiTaskModel, train_model, save_model

class FoulTrainingPipeline:
    """Pipeline for training the foul detection model."""
    
    def __init__(self, base_dir: str = 'data/dataset/', model_type: str = 'r3d_18'):
        self.base_dir = Path(base_dir)
        self.preprocessor = FoulDataPreprocessor()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.feature_extractor = FeatureExtractor(base_dir=base_dir, model_type=model_type)
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
    def extract_features(self, split: str, max_actions: Optional[int] = None) -> str:
        """Extract features for a specific dataset split."""
        return self.feature_extractor.extract_features(split, max_actions)

    def train(self, train_file: str, valid_file: str, epochs: int = 100, 
              batch_size: int = 64, learning_rate: float = 0.0005) -> ImprovedMultiTaskModel:
        """Train the model using the specified training and validation data."""
        logging.info("Starting model training...")
        
        # Process training and validation data
        X_train, y_train = self.preprocessor.process_data(train_file)
        X_val, y_val = self.preprocessor.process_data(valid_file)
        
        if X_train is None or X_val is None:
            raise ValueError("Failed to process training or validation data")
            
        # Calculate class weights from training data only
        class_weights = self._calculate_class_weights(y_train)
        
        # Get input scaler if preprocessing includes scaling
        scaler = getattr(self.preprocessor, 'scaler', None)
        
        # Train model
        model, history = train_model(
            X_train=X_train,
            y_train=y_train,
            X_val=X_val,
            y_val=y_val,
            class_weights=class_weights,
            severity_classes=len(self.preprocessor.severity_map),
            epochs=epochs,
            batch_size=batch_size,
            learning_rate=learning_rate
        )
        
        # Create model save directory with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = Path("pretrained_models") / timestamp
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # Save the model with all components
        model_path = save_dir / "foul_detection_model.pth"
        save_model(
            model=model,
            file_path=str(model_path),
            class_weights=class_weights,
            training_history=history,
            scaler=scaler
        )
        
        # Save additional metadata for reference
        self._save_mapping_info(save_dir)
        
        logging.info(f"Model and metadata saved to {save_dir}")
        return model

    def _calculate_class_weights(self, y_train: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """Calculate class weights for all tasks."""
        return {
            'actionclass': self.preprocessor.get_class_weights(
                y_train['actionclass'], len(self.preprocessor.action_class_map)),
            'bodypart': self.preprocessor.get_class_weights(
                y_train['bodypart'], len(self.preprocessor.bodypart_map)),
            'offence': self.preprocessor.get_class_weights(
                y_train['offence'], len(self.preprocessor.offence_map)),
            'touchball': self.preprocessor.get_class_weights(
                y_train['touchball'], len(self.preprocessor.touchball_map)),
            'trytoplay': self.preprocessor.get_class_weights(
                y_train['trytoplay'], len(self.preprocessor.trytoplay_map)),
            'severity': self.preprocessor.get_class_weights(
                y_train['severity'], len(self.preprocessor.severity_map))
        }
    
    def _save_mapping_info(self, save_dir: Path) -> None:
        """Save label mapping information."""
        import json

        def convert_bytes_to_str(mapping):
            """Convert bytes keys/values to strings in a mapping."""
            converted = {}
            for k, v in mapping.items():
                # Convert key from bytes to str if needed
                key = k.decode('utf-8') if isinstance(k, bytes) else str(k)
                # Convert value from bytes to str if needed
                value = v.decode('utf-8') if isinstance(v, bytes) else str(v)
                converted[key] = value
            return converted
        
        mapping_info = {
            'action_class_map': convert_bytes_to_str(self.preprocessor.action_class_map),
            'bodypart_map': convert_bytes_to_str(self.preprocessor.bodypart_map),
            'offence_map': convert_bytes_to_str(self.preprocessor.offence_map),
            'touchball_map': convert_bytes_to_str(self.preprocessor.touchball_map),
            'trytoplay_map': convert_bytes_to_str(self.preprocessor.trytoplay_map),
            'severity_map': convert_bytes_to_str(self.preprocessor.severity_map)
        }
        
        with open(save_dir / 'label_mappings.json', 'w') as f:
            json.dump(mapping_info, f, indent=2)

def main():
    """Run the training pipeline."""
    pipeline = FoulTrainingPipeline()
    
    try:
        # 1. Extract features for training and validation
        logging.info("Extracting features...")
        # train_features = pipeline.extract_features('train', max_actions=None)
        # valid_features = pipeline.extract_features('valid', max_actions=None)
        
        train_features = 'data/dataset/train/train_features.h5'
        valid_features = 'data/dataset/valid/valid_features.h5'
        
        # 2. Train model
        logging.info("Training model...")
        model = pipeline.train(
            train_file=train_features,
            valid_file=valid_features,
            epochs=150,
            batch_size=64,
            learning_rate=0.0005
        )
        
        logging.info("Training pipeline completed successfully!")
        
    except Exception as e:
        logging.error(f"Training pipeline failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()

--- utils/FeatureExtractor.py ---
import os
import json
import cv2
import torch
import logging
import numpy as np
from PIL import Image
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from torchvision import transforms
from torchvision.models.video import (
    r3d_18, R3D_18_Weights, mc3_18, MC3_18_Weights,
    r2plus1d_18, R2Plus1D_18_Weights, s3d, S3D_Weights,
    mvit_v2_s, MViT_V2_S_Weights, mvit_v1_b, MViT_V1_B_Weights
)
from typing import List, Dict, Union, Optional
from models.ActionData import ActionData
from utils.HDF5Reader import save_to_hdf5

class FeatureExtractor:
    def __init__(self, base_dir: str = 'data/dataset/', model_type: str = 'r3d_18'):
        self.base_dir = Path(base_dir)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self._initialize_model(model_type).to(self.device).eval()
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], 
                               std=[0.22803, 0.22145, 0.216989])
        ])
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )

    def _initialize_model(self, model_type: str):
        """Initialize the model based on the specified model type."""
        model_mapping = {
            'r3d_18': (r3d_18, R3D_18_Weights.DEFAULT),
            'mc3_18': (mc3_18, MC3_18_Weights.DEFAULT),
            'r2plus1d_18': (r2plus1d_18, R2Plus1D_18_Weights.DEFAULT),
            's3d': (s3d, S3D_Weights.DEFAULT),
            'mvit_v2_s': (mvit_v2_s, MViT_V2_S_Weights.DEFAULT),
            'mvit_v1_b': (mvit_v1_b, MViT_V1_B_Weights.DEFAULT)
        }

        if model_type not in model_mapping:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        model_class, model_weights = model_mapping[model_type]
        model = model_class(weights=model_weights)
        
        # Remove the final classification layer
        if hasattr(model, 'fc'):
            model.fc = torch.nn.Identity()
        elif hasattr(model, 'classifier'):
            model.classifier = torch.nn.Identity()
        elif hasattr(model, 'head'):
            model.head = torch.nn.Identity()

        return model

    def preprocess_frame(self, frame: np.ndarray) -> torch.Tensor:
        """Preprocess a single frame."""
        return self.transform(Image.fromarray(frame))

    def preprocess_frames(self, frames: List[np.ndarray]) -> torch.Tensor:
        """Preprocess frames concurrently."""
        with ThreadPoolExecutor() as executor:
            processed_frames = list(executor.map(self.preprocess_frame, frames))

        frames_tensor = torch.stack(processed_frames)
        return frames_tensor.permute(1, 0, 2, 3)

    def extract_video_features(self, video_path: str) -> Optional[torch.Tensor]:
        """Extract features from a single video file."""
        if not os.path.exists(video_path):
            logging.error(f"Video file not found: {video_path}")
            return None

        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logging.error(f"Failed to open video file: {video_path}")
                return None

            frames = []
            # Capture frames 63 to 87
            for frame_idx in range(63, 88):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if not ret:
                    logging.warning(f"Failed to read frame {frame_idx}")
                    break
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)

            cap.release()

            if len(frames) != 25:
                logging.error(f"Expected 25 frames, but got {len(frames)}")
                return None

            frames_tensor = self.preprocess_frames(frames).unsqueeze(0).to(self.device)

            with torch.no_grad():
                features = self.model(frames_tensor)

            return features

        except Exception as e:
            logging.error(f"Error extracting features for {video_path}: {str(e)}")
            return None

    def extract_clip_features(self, action: ActionData) -> None:
        """Extract features from all clips of an action concurrently."""
        logging.info(f"Starting feature extraction for action with {len(action.clips)} clips")

        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = []
            
            for clip in action.clips:
                video_path = os.path.join('data', clip['Url'].lower() + '.mp4')
                futures.append(executor.submit(self._process_single_clip, video_path, clip))
            
            # Wait for all clips to complete and handle results
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logging.error(f"Error during feature extraction: {str(e)}")

    def _process_single_clip(self, video_path: str, clip: Dict) -> None:
        """Process a single clip and store its features."""
        logging.info(f"Extracting features for video: {video_path}")
        clip['video_features'] = self.extract_video_features(video_path)

    def extract_features(self, split: str, max_actions: Optional[int] = None) -> str:
        """Extract features for a specific dataset split."""
        input_file = self.base_dir / split / 'annotations.json'
        output_file = self.base_dir / split / f'{split}_features.h5'
        
        if not input_file.exists():
            raise FileNotFoundError(f"Annotations file not found: {input_file}")
            
        # Load annotations
        with open(input_file, 'r') as f:
            annotations = json.load(f)
            
        logging.info(f"Dataset Set: {annotations['Set']}")
        logging.info(f"Total Actions: {annotations['Number of actions']}")
        
        # Process one action at a time
        actions = []
        action_count = 0
        
        for action_id, action_data in annotations['Actions'].items():
            if max_actions and action_count >= max_actions:
                break
                
            logging.info(f"Processing Action ID: {action_id}")
            action = ActionData(action_data)
            
            if action.valid:
                # Process all clips for this action concurrently
                self.extract_clip_features(action)
                actions.append(action)
            else:
                logging.info(f"Skipped Action ID: {action_id}")
            
            action_count += 1
        
        # Save extracted features
        save_to_hdf5(actions, str(output_file))
        logging.info(f"Saved features to {output_file}")
        
        return str(output_file)

def main():
    """Extract features for training and validation sets."""
    extractor = FeatureExtractor()
    
    try:
        # Extract features for training and validation
        train_features = extractor.extract_features('train')
        valid_features = extractor.extract_features('valid')
        test_features = extractor.extract_features('test')
        
        logging.info("Feature extraction completed successfully!")
        
    except Exception as e:
        logging.error(f"Feature extraction failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()

--- utils/FoulDataPreprocessor.py ---
import os
import logging
import torch
from utils.HDF5Reader import read_from_hdf5

class FoulDataPreprocessor:
    def __init__(self):
        self.action_class_map = {
            b'Standing tackling': 0, b'Tackling': 1, b'Holding': 2,
            b'Challenge': 3, b'Elbowing': 4, b'High leg': 5,
            b'Pushing': 6, b'Dive': 7
        }
        
        self.bodypart_map = {
            b'Under body': 0, b'Use of arms': 1,
            b'Use of shoulder': 2, b'Upper body': 3
        }
        
        self.offence_map = {
            b'Offence': 2, b'No offence': 0, b'Between': 1
        }
        
        self.touchball_map = {
            b'No': 0, b'Yes': 2, b'Maybe': 1
        }
        
        self.trytoplay_map = {
            b'No': 0, b'Yes': 1
        }
        
        self.severity_map = {
            b'1.0': 0,
            b'2.0': 1,
            b'3.0': 2,
            b'4.0': 3,
            b'5.0': 4
        }
        
        self.target_camera = b'Close-up player or field referee'

    def get_class_weights(self, labels, num_classes):
        """Calculate class weights for imbalanced classes."""
        counts = torch.bincount(labels, minlength=num_classes)
        total = len(labels)
        weights = total / (counts * num_classes)
        return weights

    def is_valid_features(self, video_features):
        """Check if video features are valid (non-empty and non-zero)."""
        return (video_features is not None and 
                isinstance(video_features, torch.Tensor) and 
                video_features.numel() > 0 and 
                not torch.all(video_features == 0))

    def encode_labels(self, action):
        """Encode categorical variables into numerical labels."""
        return {
            'actionclass': self.action_class_map[action['actionclass']],
            'bodypart': self.bodypart_map[action['bodypart']],
            'offence': self.offence_map[action['offence']],
            'severity': self.severity_map[action['severity']],
            'touchball': self.touchball_map[action['touchball']],
            'trytoplay': self.trytoplay_map[action['trytoplay']]
        }
    
    def process_data(self, input_file):
        """Process and reshape the data for deep learning."""
        if not os.path.exists(input_file):
            logging.error(f"File not found: {input_file}")
            return None
            
        actions = read_from_hdf5(input_file)
        
        features = []
        labels = []
        
        # Track statistics
        total_actions = len(actions)
        processed_actions = 0
        skipped_no_target_camera = 0
        skipped_empty_features = 0
        
        for action_idx, action in enumerate(actions):
            # Track target camera clips
            target_camera_count = 0
            valid_features = []

            # Process clips
            for clip in action['clips']:
                if clip['Camera type'] == self.target_camera:
                    target_camera_count += 1
                    
                    # Skip empty features
                    if not self.is_valid_features(clip['video_features']):
                        skipped_empty_features += 1
                        continue
                    
                    # Extract and combine features
                    video_features = clip['video_features'].squeeze()
                    replay_speed = torch.tensor([float(clip['Replay speed'])])
                    combined_features = torch.cat([video_features, replay_speed])
                    valid_features.append(combined_features)
            
            # Log target camera count
            # logging.info(f"Action {action_idx + 1}/{total_actions}: {target_camera_count} target camera clips.")

            # Check if action is valid
            if not valid_features:
                skipped_no_target_camera += 1
                continue

            processed_actions += 1
            encoded_labels = self.encode_labels(action)
            features.extend(valid_features)
            labels.extend([encoded_labels] * len(valid_features))
        
        # Log statistics
        logging.info(f"\nProcessing Summary:")
        logging.info(f"Total actions: {total_actions}")
        logging.info(f"Processed actions: {processed_actions}")
        logging.info(f"Skipped actions (no target camera): {skipped_no_target_camera}")
        logging.info(f"Skipped clips (empty features): {skipped_empty_features}")
        
        if not features:
            logging.error("No valid features found in the dataset")
            return None
            
        # Stack features and convert labels
        X = torch.stack(features)
        y = {
            'actionclass': torch.tensor([label['actionclass'] for label in labels]),
            'bodypart': torch.tensor([label['bodypart'] for label in labels]),
            'offence': torch.tensor([label['offence'] for label in labels]),
            'severity': torch.tensor([label['severity'] for label in labels]),
            'touchball': torch.tensor([label['touchball'] for label in labels]),
            'trytoplay': torch.tensor([label['trytoplay'] for label in labels])
        }
        
        logging.info(f"Final dataset shape: {X.shape}")
        logging.info(f"Features per action: {len(features) / processed_actions:.2f}")
        return X, y

def main():
    logging.basicConfig(level=logging.INFO)
    preprocessor = FoulDataPreprocessor()
    input_file = 'data/dataset/train/train_features.h5'
    
    X, y = preprocessor.process_data(input_file)
    
    if X is not None:
        # Calculate class weights for each task
        class_weights = {
            'actionclass': preprocessor.get_class_weights(y['actionclass'], len(preprocessor.action_class_map)),
            'bodypart': preprocessor.get_class_weights(y['bodypart'], len(preprocessor.bodypart_map)),
            'offence': preprocessor.get_class_weights(y['offence'], len(preprocessor.offence_map)),
            'touchball': preprocessor.get_class_weights(y['touchball'], len(preprocessor.touchball_map)),
            'trytoplay': preprocessor.get_class_weights(y['trytoplay'], len(preprocessor.trytoplay_map)),
            'severity': preprocessor.get_class_weights(y['severity'], len(preprocessor.severity_map))
        }
        return X, y, class_weights

if __name__ == "__main__":
    main()


--- utils/HDF5Reader.py ---
import os
import h5py
import torch
import logging
import numpy as np
from collections import Counter

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def save_to_hdf5(actions, output_file):
    """
    Saves action data and video features to an HDF5 file.
    """
    with h5py.File(output_file, 'w') as f:
        for idx, action in enumerate(actions):
            action_group = f.create_group(f"action_{idx}")

            # Save attributes (non-clip data)
            for attr in vars(action):
                if attr != "clips":
                    action_group.create_dataset(attr, data=getattr(action, attr))

            # Save clips
            clips_group = action_group.create_group("clips")
            for clip_idx, clip in enumerate(action.clips):
                clip_group = clips_group.create_group(f"clip_{clip_idx}")

                # Save non-video features
                for key, value in clip.items():
                    if key != 'video_features':
                        clip_group.create_dataset(key, data=value)

                # Save video features
                if clip['video_features'] is not None:
                    video_features_group = clip_group.create_group('video_features')
                    video_features_np = clip['video_features'].cpu().numpy()
                    video_features_group.create_dataset('data', data=video_features_np)

    logging.info(f"Saved action data to {output_file}")

def print_field_occurrences(actions):
    """
    Logs distinct occurrences and counts for each field in the actions data.
    """
    if not actions:
        logging.warning("No actions provided for analysis.")
        return

    fields = [field for field in actions[0].keys() if field != 'clips']
    logging.info("Field value distributions:")

    for field in sorted(fields):
        value_counts = Counter(str(action[field]) for action in actions)
        logging.info(f"\n{field} - Total entries: {len(actions)}")
        logging.info("-" * 50)

        for value, count in sorted(value_counts.items(), key=lambda x: -x[1]):
            percentage = (count / len(actions)) * 100
            logging.info(f"'{value}': {count} occurrences ({percentage:.1f}%)")

def read_from_hdf5(input_file):
    """
    Reads action data and video features from an HDF5 file.
    """
    with h5py.File(input_file, 'r') as f:
        actions = []
        for action_key in f.keys():
            action_group = f[action_key]
            action_data = {attr: action_group[attr][()] for attr in action_group if attr != 'clips'}

            # Read clips
            clips = []
            if 'clips' in action_group:
                clips_group = action_group['clips']
                for clip_key in clips_group.keys():
                    clip_group = clips_group[clip_key]
                    clip_data = {key: clip_group[key][()] for key in clip_group if key != 'video_features'}

                    # Read video features
                    video_features = None
                    if 'video_features' in clip_group:
                        features_group = clip_group['video_features']
                        if 'data' in features_group:
                            video_features_np = features_group['data'][()]
                            video_features = torch.tensor(video_features_np)

                    clip_data['video_features'] = video_features
                    clips.append(clip_data)

            action_data['clips'] = clips
            actions.append(action_data)

        logging.info(f"Loaded {len(actions)} actions from {input_file}")
        return actions

def main():
    """
    Main function to load and inspect HDF5 data.
    """
    input_file = 'data/dataset/test/test_features.h5'

    if os.path.exists(input_file):
        actions = read_from_hdf5(input_file)
        
        for index, action in enumerate(actions):
            logging.info(f"Action ID: {index}")
            logging.info(f"Body part: {action['bodypart']}")
            for clip in action['clips']:
                camera_angle = clip['Camera type']
                video_features = clip['video_features']
                
        print_field_occurrences(actions)
    else:
        logging.error(f"File not found: {input_file}")

if __name__ == "__main__":
    main()


--- utils/training.py ---
import torch
import logging
import torch.nn as nn
import torch.optim as optim
from matplotlib import pyplot as plt
import datetime
from collections import defaultdict
import math
import json
from torch.utils.data import DataLoader, Dataset
from utils.FoulDataPreprocessor import FoulDataPreprocessor

class TaskSpecificHead(nn.Module):
    def __init__(self, input_size, output_size, hidden_size=None, dropout_rate=0.5):
        super().__init__()
        if hidden_size is None:
            hidden_size = max(output_size * 2, input_size // 2)
        
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.GELU(),  # Changed to GELU for better gradient flow
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, output_size)
        )
    
    def forward(self, x):
        return self.net(x)

class ImprovedMultiTaskModel(nn.Module):
    def __init__(self, input_size, action_classes, bodypart_classes, offence_classes, 
                 touchball_classes, trytoplay_classes, severity_classes, dropout_rate=0.5):
        super().__init__()
        
        # Increased capacity in shared layers
        self.shared_net = nn.Sequential(
            nn.Linear(input_size, 768),
            nn.LayerNorm(768),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(768, 384),
            nn.LayerNorm(384),
            nn.GELU(),
            nn.Dropout(dropout_rate)
        )
        
        # Primary task heads
        self.primary_heads = nn.ModuleDict({
            'actionclass': TaskSpecificHead(384, action_classes, hidden_size=512),
            'bodypart': TaskSpecificHead(384, bodypart_classes, hidden_size=512),
            'offence': TaskSpecificHead(384, offence_classes, hidden_size=384),
            'touchball': TaskSpecificHead(384, touchball_classes, hidden_size=384),
            'trytoplay': TaskSpecificHead(384, trytoplay_classes, hidden_size=256)
        })
        
        # Severity prediction using other task outputs
        total_task_outputs = (action_classes + bodypart_classes + offence_classes + 
                            touchball_classes + trytoplay_classes)
        
        self.severity_head = nn.Sequential(
            nn.Linear(384 + total_task_outputs, 512),  # Concatenated features + task outputs
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 384),
            nn.LayerNorm(384),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(384, severity_classes)
        )
        
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        shared_features = self.shared_net(x)
        
        # Get primary task outputs
        primary_outputs = {
            task: head(shared_features) for task, head in self.primary_heads.items()
        }
        
        # Concatenate all task logits with shared features for severity prediction
        task_logits = torch.cat([primary_outputs[task] for task in self.primary_heads.keys()], dim=1)
        severity_input = torch.cat([shared_features, task_logits], dim=1)
        
        # Add severity prediction
        primary_outputs['severity'] = self.severity_head(severity_input)
        
        return primary_outputs
    
class MultiTaskDataset(Dataset):
    def __init__(self, X, y_dict):
        self.X = torch.FloatTensor(X)
        self.y_dict = {task: torch.LongTensor(labels) for task, labels in y_dict.items()}
        self.tasks = list(y_dict.keys())

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x = self.X[idx]
        y = {task: self.y_dict[task][idx] for task in self.tasks}
        return x, y

def plot_training_history(history, save_path):
    """
    Plot and save training and validation losses.
    
    Args:
        history: Dictionary containing training history
        save_path: Path to save the plot
    """
    plt.figure(figsize=(15, 10))
    
    # Plot task-specific losses
    for idx, task in enumerate(history['train_losses'].keys(), 1):
        plt.subplot(3, 2, idx)
        plt.plot(history['train_losses'][task], label='Train')
        plt.plot(history['val_losses'][task], label='Validation')
        plt.title(f'{task.capitalize()} Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
    
    # Adjust layout and save
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def train_model(X_train, y_train, X_val, y_val, class_weights, severity_classes, 
                epochs=100, batch_size=128, learning_rate=0.0003):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. Gradient Accumulation for larger effective batch size
    accum_iter = 4
    effective_batch_size = batch_size * accum_iter
    
    # 2. Create datasets with weighted sampling
    train_weights = compute_sample_weights(y_train)
    train_sampler = torch.utils.data.WeightedRandomSampler(
        train_weights, len(train_weights), replacement=True
    )
    
    train_dataset = MultiTaskDataset(X_train, y_train)
    val_dataset = MultiTaskDataset(X_val, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    # 3. Model with LayerNorm and residual connections
    model = ImprovedMultiTaskModel(
        input_size=X_train.shape[1],
        action_classes=len(class_weights['actionclass']),
        bodypart_classes=len(class_weights['bodypart']),
        offence_classes=len(class_weights['offence']),
        touchball_classes=len(class_weights['touchball']),
        trytoplay_classes=len(class_weights['trytoplay']),
        severity_classes=severity_classes
    ).to(device)
    
    # 4. Improved learning rate scheduling
    base_lr = learning_rate
    warmup_epochs = 5
    
    # 5. Automatic Mixed Precision for faster training
    scaler = torch.cuda.amp.GradScaler()
    
    # 6. Dynamic task weighting based on loss magnitudes
    task_weights = {task: 1.0 for task in class_weights.keys()}
    
    # 7. Label smoothing in loss function
    criteria = {
        task: FocalLoss(
            weight=class_weights[task].to(device),
            gamma=2 if task in ['actionclass', 'severity'] else 1,
            label_smoothing=0.1
        ) for task in class_weights.keys()
    }
    
    # 8. Layer-wise learning rates
    param_groups = [
        {'params': model.shared_net.parameters(), 'lr': base_lr},
        {'params': model.primary_heads.parameters(), 'lr': base_lr * 1.5},
        {'params': model.severity_head.parameters(), 'lr': base_lr * 2.0}
    ]
    
    optimizer = optim.AdamW(param_groups, weight_decay=0.01)
    
    # 9. Cosine annealing with warmup
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        return 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # 10. Improved early stopping with loss plateau detection
    best_val_loss = float('inf')
    patience = 15
    plateau_threshold = 0.001
    plateau_count = 0
    best_model_state = None
    
    history = {
        'train_losses': {task: [] for task in class_weights.keys()},
        'val_losses': {task: [] for task in class_weights.keys()},
        'lrs': []
    }
    
    for epoch in range(epochs):
        # Training phase with gradient accumulation
        model.train()
        train_losses = defaultdict(float)
        optimizer.zero_grad()
        
        for i, (inputs, labels_dict) in enumerate(train_loader):
            inputs = inputs.to(device)
            labels_dict = {k: v.to(device) for k, v in labels_dict.items()}
            
            # Use automatic mixed precision
            with torch.cuda.amp.autocast():
                outputs = model(inputs)
                batch_loss = 0
                for task, criterion in criteria.items():
                    task_loss = criterion(outputs[task], labels_dict[task])
                    weighted_loss = task_loss * task_weights[task]
                    batch_loss += weighted_loss
                    train_losses[task] += task_loss.item()
                
                # Scale loss by accumulation factor
                batch_loss = batch_loss / accum_iter
            
            # Accumulate gradients
            scaler.scale(batch_loss).backward()
            
            if (i + 1) % accum_iter == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
        
        # Validation phase
        val_losses = validate_epoch(model, val_loader, criteria, task_weights, device)
        
        # Update learning rate
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        history['lrs'].append(current_lr)
        
        # Dynamic task weight adjustment
        val_losses_tensor = torch.tensor([val_losses[task] for task in criteria.keys()])
        max_loss = val_losses_tensor.max().item()
        
        for task in task_weights:
            if task == 'severity':
                min_other_loss = min([val_losses[t] for t in criteria.keys() if t != 'severity'])
                task_weights[task] = (val_losses[task] / max_loss) * (1.5 if min_other_loss < 0.1 else 1.0)
            else:
                task_weights[task] = (val_losses[task] / max_loss)
        
        # Update history
        for task in criteria.keys():
            history['train_losses'][task].append(train_losses[task] / len(train_loader))
            history['val_losses'][task].append(val_losses[task])
        
        # Early stopping with plateau detection
        val_loss = sum(val_losses.values())
        if val_loss < best_val_loss - plateau_threshold:
            best_val_loss = val_loss
            best_model_state = model.state_dict()
            plateau_count = 0
        else:
            plateau_count += 1
            
        if plateau_count >= patience:
            print(f"Training stopped at epoch {epoch + 1} due to loss plateau")
            break
        
        # Logging
        if (epoch + 1) % 5 == 0:
            print(f"Epoch [{epoch + 1}/{epochs}] LR: {current_lr:.6f}")
            for task in criteria.keys():
                print(f"{task}: Train = {train_losses[task]/len(train_loader):.4f}, "
                      f"Val = {val_losses[task]:.4f}, Weight = {task_weights[task]:.3f}")
    
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    return model, history

def train_epoch(model, train_loader, criteria, optimizer, scheduler, task_weights, device):
    """Run one epoch of training."""
    model.train()
    train_task_losses = {task: 0.0 for task in criteria.keys()}
    num_batches = 0
    
    for inputs, labels_dict in train_loader:
        inputs = inputs.to(device)
        labels_dict = {k: v.to(device) for k, v in labels_dict.items()}
        
        optimizer.zero_grad()
        outputs = model(inputs)
        
        # Compute weighted task losses
        batch_loss = 0
        for task, criterion in criteria.items():
            task_loss = criterion(outputs[task], labels_dict[task])
            weighted_loss = task_loss * task_weights[task]
            batch_loss += weighted_loss
            train_task_losses[task] += task_loss.item()
        
        batch_loss.backward()
        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        
        num_batches += 1
    
    # Average losses over batches
    return {task: loss / num_batches for task, loss in train_task_losses.items()}

def validate_epoch(model, val_loader, criteria, task_weights, device):
    """Run one epoch of validation."""
    model.eval()
    val_task_losses = {task: 0.0 for task in criteria.keys()}
    num_batches = 0
    
    with torch.no_grad():
        for inputs, labels_dict in val_loader:
            inputs = inputs.to(device)
            labels_dict = {k: v.to(device) for k, v in labels_dict.items()}
            outputs = model(inputs)
            
            for task, criterion in criteria.items():
                task_loss = criterion(outputs[task], labels_dict[task])
                val_task_losses[task] += task_loss.item()
            num_batches += 1
    
    # Average losses over batches
    return {task: loss / num_batches for task, loss in val_task_losses.items()}

class FocalLoss(nn.Module):
    def __init__(self, weight=None, gamma=2.0, label_smoothing=0.0):
        super().__init__()
        self.gamma = gamma
        self.weight = weight
        self.label_smoothing = label_smoothing
        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none', label_smoothing=label_smoothing)
        
    def forward(self, input, target):
        ce_loss = self.ce(input, target)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()
        return focal_loss

def compute_sample_weights(y_dict):
    """Compute sample weights based on task difficulty and class distribution"""
    weights = torch.ones(len(next(iter(y_dict.values()))))
    
    for task, labels in y_dict.items():
        # Convert to tensor if necessary
        if not isinstance(labels, torch.Tensor):
            labels = torch.tensor(labels)
            
        # Count class frequencies
        unique, counts = torch.unique(labels, return_counts=True)
        class_weights = 1.0 / counts.float()
        class_weights = class_weights / class_weights.sum()
        
        # Apply weights to samples
        sample_weights = class_weights[labels]
        weights *= sample_weights
    
    # Normalize weights
    weights = weights / weights.sum() * len(weights)
    return weights

def save_model(model, file_path, class_weights=None, training_history=None, scaler=None):
    """
    Save the model along with metadata and optional training artifacts.
    """
    # Get base path without extension
    base_path = file_path.rsplit('.', 1)[0]
    
    # Save the training history plot if history exists
    if training_history is not None:
        plot_path = f"{base_path}_training_history.png"
        plot_training_history(training_history, plot_path)
        print(f"Training history plot saved to {plot_path}")
    
    # Get model configuration
    metadata = {
        'model_config': {
            'input_size': model.shared_net[0].in_features,
            'action_classes': model.primary_heads['actionclass'].net[-1].out_features,
            'bodypart_classes': model.primary_heads['bodypart'].net[-1].out_features,
            'offence_classes': model.primary_heads['offence'].net[-1].out_features,
            'touchball_classes': model.primary_heads['touchball'].net[-1].out_features,
            'trytoplay_classes': model.primary_heads['trytoplay'].net[-1].out_features,
            'severity_classes': model.severity_head[-1].out_features
        },
        'architecture_version': '2.0',
        'timestamp': datetime.datetime.now().isoformat()
    }
    
    # Prepare checkpoint
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'metadata': metadata,
        'class_weights': class_weights,
        'training_history': training_history
    }
    
    if scaler is not None:
        checkpoint['scaler'] = scaler
    
    # Save to file
    torch.save(checkpoint, file_path)
    print(f"Model saved to {file_path}")
    print("Saved metadata:", json.dumps(metadata, indent=2))

def load_model(file_path, device=None):
    """
    Load the model and associated metadata.
    
    Args:
        file_path: Path to the saved model
        device: Optional torch device to load the model to
        
    Returns:
        model: Loaded model
        metadata: Model metadata
        class_weights: Class weights used during training
        training_history: Training metrics history
        scaler: Data scaler if it was saved, None otherwise
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load checkpoint
    checkpoint = torch.load(file_path, map_location=device)
    metadata = checkpoint['metadata']
    
    # Version compatibility check
    if metadata.get('architecture_version', '1.0') != '2.0':
        print("Warning: Loading model from different architecture version. Some features might not be available.")
    
    # Initialize model with saved configuration
    model = ImprovedMultiTaskModel(
        **metadata['model_config']
    ).to(device)
    
    # Load model weights
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"Model loaded from {file_path}")
    print("Loaded metadata:", json.dumps(metadata, indent=2))
    
    # Return all saved components
    return (
        model,
        metadata,
        checkpoint.get('class_weights'),
        checkpoint.get('training_history'),
        checkpoint.get('scaler')
    )


def main():
    logging.basicConfig(level=logging.INFO)
    preprocessor = FoulDataPreprocessor()
    input_file = 'data/dataset/train/train_features.h5'
    
    X_train, y_train = preprocessor.process_data(input_file)
    # TODO: Add validation data processing
    
    if X_train is not None:
        # Calculate class weights for each task, including severity
        class_weights = {
            'actionclass': preprocessor.get_class_weights(y_train['actionclass'], len(preprocessor.action_class_map)),
            'bodypart': preprocessor.get_class_weights(y_train['bodypart'], len(preprocessor.bodypart_map)),
            'offence': preprocessor.get_class_weights(y_train['offence'], len(preprocessor.offence_map)),
            'touchball': preprocessor.get_class_weights(y_train['touchball'], len(preprocessor.touchball_map)),
            'trytoplay': preprocessor.get_class_weights(y_train['trytoplay'], len(preprocessor.trytoplay_map)),
            'severity': preprocessor.get_class_weights(y_train['severity'], len(preprocessor.severity_map))  # Add severity class weights
        }
    
    model = train_model(X_train, y_train, class_weights, severity_classes=len(preprocessor.severity_map), epochs=100, batch_size=64, learning_rate=0.0005)
    
    metadata = {
        'input_size': X_train.shape[1],
        'action_classes': len(class_weights['actionclass']),
        'bodypart_classes': len(class_weights['bodypart']),
        'offence_classes': len(class_weights['offence']),
        'touchball_classes': len(class_weights['touchball']),
        'trytoplay_classes': len(class_weights['trytoplay']),
        'severity_classes': len(class_weights['severity'])  # Add severity to metadata
    }
    
    save_model(model, "foul_detection_model.pth", metadata)

if __name__ == "__main__":
    main()

